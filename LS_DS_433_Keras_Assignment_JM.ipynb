{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_433_Keras_Assignment_JM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayTheOpenSourcerer/DS-Unit-4-Sprint-2-Neural-Networks/blob/master/LS_DS_433_Keras_Assignment_JM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pBQsZEJmubLs"
      },
      "source": [
        "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
        "\n",
        "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
        "- Normalize the data (all features should have roughly the same scale)\n",
        "- Import the type of model and layers that you will need from Keras.\n",
        "- Instantiate a model object and use `model.add()` to add layers to your model\n",
        "- Since this is a regression model you will have a single output node in the final layer.\n",
        "- Use activation functions that are appropriate for this task\n",
        "- Compile your model\n",
        "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
        "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
        "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NLTAR87uYJ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1ae8a78d-e056-474d-f448-9109847e82a5"
      },
      "source": [
        "from keras.datasets import boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1XX5Vyg1N2m",
        "colab_type": "code",
        "outputId": "f83c9719-277f-4e96-9989-c967a165f3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "x_train = pd.DataFrame(x_train)\n",
        "y_train = pd.DataFrame(y_train)\n",
        "x_test = pd.DataFrame(x_test)\n",
        "y_test = pd.DataFrame(y_test)\n",
        "\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((404, 13), (404, 1), (102, 13), (102, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTgWIxap1N26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "x_train = scaler.fit_transform(x_train, y_train)\n",
        "x_test = scaler.fit_transform(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPnJSp2n1N3V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7232ebfd-81a5-4459-8234-c07899eee0e1"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "model = Sequential()\n",
        "# input layer\n",
        "model.add(Dense(1, input_dim=13, activation='sigmoid'))\n",
        "# hidden layet\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "#output layer\n",
        "model.add(Dense(1, activation=\"linear\"))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=\"adam\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0719 01:26:18.927739 139707300788096 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foQ72mfq1N3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=500, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gdk4H9g1N3z",
        "colab_type": "code",
        "outputId": "4030a746-49d9-44c6-d409-c4c75e9f5112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "scores = model.evaluate(x_test, y_test)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102/102 [==============================] - 0s 335us/sample - loss: 203.6673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjNB6ieM1N4V",
        "colab_type": "code",
        "outputId": "4f28a2a2-7214-4870-dfc0-ec22dc6536c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUAbnNPp1N4n",
        "colab_type": "code",
        "outputId": "a60994b1-4e3e-45c1-a185-04dfd2f9a267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(history.history['loss'][0:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[581.9947400990098, 580.9842783106436, 579.9646491626702, 578.9248814346766, 577.8880687751392, 576.8403870233215, 575.7704963306389, 574.7094224986464, 573.629204778388, 572.5504380027846]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1SC8j3G1N4x",
        "colab_type": "code",
        "outputId": "85e7c4c0-ce88-40bb-b049-c05eafa77fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkWIpIEREpFj/D3MnrY8Yb0yYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gKMA4p91N5B",
        "colab_type": "code",
        "outputId": "76c8dfbf-ef9d-440e-fa20-aef97e0812a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(x_train, y_train)\n",
        "y_pred = lin_reg.predict(x_test)\n",
        "\n",
        "mean_squared_error(y_test, y_pred)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.60145446671504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SfcFnOONyuNm"
      },
      "source": [
        "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
        "\n",
        "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
        "- Make sure to one-hot encode your category labels\n",
        "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
        "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
        "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
        "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "szi6-IpuzaH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e67b0570-26da-4119-c60f-92065336b9fb"
      },
      "source": [
        "import tensorflow as tf\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zc4n37Q1N5g",
        "colab_type": "code",
        "outputId": "6a4e3e19-2af0-4bec-d5ab-52058705f3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laWyePX51N5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRCpKff31N53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set to floats\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKCqN0K31N6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set outputs to categorical\n",
        "import keras\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxT3MPNE1N6m",
        "colab_type": "code",
        "outputId": "60c51fc1-9145-40bf-ec1a-d7878cc24276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky9z_5QI1N6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_model = Sequential()\n",
        "\n",
        "# hidden layer\n",
        "mnist_model.add(Dense(16, input_dim=784, activation='relu'))\n",
        "mnist_model.add(Dense(16, activation='relu'))\n",
        "# output layer\n",
        "mnist_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "mnist_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFlxWQy71N7S",
        "colab_type": "code",
        "outputId": "4a26289b-b7e8-4010-cc67-3893e1362bab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=150, verbose=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 2.1891 - acc: 0.3080\n",
            "Epoch 2/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 1.1980 - acc: 0.5321\n",
            "Epoch 3/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.9175 - acc: 0.6533\n",
            "Epoch 4/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.7223 - acc: 0.7536\n",
            "Epoch 5/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.5569 - acc: 0.8428\n",
            "Epoch 6/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.4936 - acc: 0.8629\n",
            "Epoch 7/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.4596 - acc: 0.8713\n",
            "Epoch 8/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.4236 - acc: 0.8799\n",
            "Epoch 9/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.3993 - acc: 0.8860\n",
            "Epoch 10/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.3590 - acc: 0.8970\n",
            "Epoch 11/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.3386 - acc: 0.9022\n",
            "Epoch 12/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.3269 - acc: 0.9061\n",
            "Epoch 13/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.3161 - acc: 0.9079\n",
            "Epoch 14/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.3090 - acc: 0.9100\n",
            "Epoch 15/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.3024 - acc: 0.9116\n",
            "Epoch 16/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2990 - acc: 0.9126\n",
            "Epoch 17/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2932 - acc: 0.9133\n",
            "Epoch 18/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2896 - acc: 0.9147\n",
            "Epoch 19/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2878 - acc: 0.9147\n",
            "Epoch 20/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2855 - acc: 0.9150\n",
            "Epoch 21/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2809 - acc: 0.9176\n",
            "Epoch 22/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2799 - acc: 0.9170\n",
            "Epoch 23/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2770 - acc: 0.9172\n",
            "Epoch 24/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2746 - acc: 0.9184\n",
            "Epoch 25/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2706 - acc: 0.9194\n",
            "Epoch 26/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2722 - acc: 0.9194\n",
            "Epoch 27/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2732 - acc: 0.9192\n",
            "Epoch 28/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2690 - acc: 0.9194\n",
            "Epoch 29/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2681 - acc: 0.9215\n",
            "Epoch 30/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2670 - acc: 0.9212\n",
            "Epoch 31/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2651 - acc: 0.9215\n",
            "Epoch 32/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2645 - acc: 0.9219\n",
            "Epoch 33/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2626 - acc: 0.9220\n",
            "Epoch 34/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2628 - acc: 0.9224\n",
            "Epoch 35/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2633 - acc: 0.9218\n",
            "Epoch 36/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2604 - acc: 0.9223\n",
            "Epoch 37/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2643 - acc: 0.9221\n",
            "Epoch 38/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2640 - acc: 0.9209\n",
            "Epoch 39/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2592 - acc: 0.9233\n",
            "Epoch 40/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2606 - acc: 0.9221\n",
            "Epoch 41/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2591 - acc: 0.9241\n",
            "Epoch 42/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2570 - acc: 0.9234\n",
            "Epoch 43/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2594 - acc: 0.9239\n",
            "Epoch 44/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2562 - acc: 0.9237\n",
            "Epoch 45/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2587 - acc: 0.9229\n",
            "Epoch 46/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2587 - acc: 0.9238\n",
            "Epoch 47/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2555 - acc: 0.9233\n",
            "Epoch 48/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2555 - acc: 0.9243\n",
            "Epoch 49/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2574 - acc: 0.9237\n",
            "Epoch 50/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2541 - acc: 0.9249\n",
            "Epoch 51/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2557 - acc: 0.9245\n",
            "Epoch 52/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2559 - acc: 0.9235\n",
            "Epoch 53/150\n",
            "60000/60000 [==============================] - 3s 45us/sample - loss: 0.2545 - acc: 0.9255\n",
            "Epoch 54/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2553 - acc: 0.9244\n",
            "Epoch 55/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2529 - acc: 0.9251\n",
            "Epoch 56/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2548 - acc: 0.9248\n",
            "Epoch 57/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2523 - acc: 0.9257\n",
            "Epoch 58/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2509 - acc: 0.9252\n",
            "Epoch 59/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2523 - acc: 0.9258\n",
            "Epoch 60/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2526 - acc: 0.9262\n",
            "Epoch 61/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2537 - acc: 0.9249\n",
            "Epoch 62/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2529 - acc: 0.9250\n",
            "Epoch 63/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2534 - acc: 0.9258\n",
            "Epoch 64/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2505 - acc: 0.9254\n",
            "Epoch 65/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2491 - acc: 0.9262\n",
            "Epoch 66/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2521 - acc: 0.9249\n",
            "Epoch 67/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2512 - acc: 0.9257\n",
            "Epoch 68/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2506 - acc: 0.9256\n",
            "Epoch 69/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2500 - acc: 0.9258\n",
            "Epoch 70/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2526 - acc: 0.9253\n",
            "Epoch 71/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2485 - acc: 0.9255\n",
            "Epoch 72/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2490 - acc: 0.9258\n",
            "Epoch 73/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2495 - acc: 0.9258\n",
            "Epoch 74/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2499 - acc: 0.9258\n",
            "Epoch 75/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2501 - acc: 0.9256\n",
            "Epoch 76/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2505 - acc: 0.9250\n",
            "Epoch 77/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2472 - acc: 0.9264\n",
            "Epoch 78/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2506 - acc: 0.9254\n",
            "Epoch 79/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2463 - acc: 0.9261\n",
            "Epoch 80/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2473 - acc: 0.9268\n",
            "Epoch 81/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2459 - acc: 0.9274\n",
            "Epoch 82/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2496 - acc: 0.9256\n",
            "Epoch 83/150\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2468 - acc: 0.9268\n",
            "Epoch 84/150\n",
            "60000/60000 [==============================] - 3s 55us/sample - loss: 0.2487 - acc: 0.9260\n",
            "Epoch 85/150\n",
            "60000/60000 [==============================] - 3s 51us/sample - loss: 0.2525 - acc: 0.9240\n",
            "Epoch 86/150\n",
            "60000/60000 [==============================] - 3s 50us/sample - loss: 0.2437 - acc: 0.9270\n",
            "Epoch 87/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2464 - acc: 0.9273\n",
            "Epoch 88/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2474 - acc: 0.9267\n",
            "Epoch 89/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2445 - acc: 0.9275\n",
            "Epoch 90/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2471 - acc: 0.9269\n",
            "Epoch 91/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2452 - acc: 0.9260\n",
            "Epoch 92/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2479 - acc: 0.9263\n",
            "Epoch 93/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2474 - acc: 0.9263\n",
            "Epoch 94/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2466 - acc: 0.9265\n",
            "Epoch 95/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2462 - acc: 0.9268\n",
            "Epoch 96/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2456 - acc: 0.9275\n",
            "Epoch 97/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2454 - acc: 0.9274\n",
            "Epoch 98/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2460 - acc: 0.9272\n",
            "Epoch 99/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2488 - acc: 0.9266\n",
            "Epoch 100/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2450 - acc: 0.9275\n",
            "Epoch 101/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2443 - acc: 0.9278\n",
            "Epoch 102/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2427 - acc: 0.9277\n",
            "Epoch 103/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2456 - acc: 0.9274\n",
            "Epoch 104/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2443 - acc: 0.9285\n",
            "Epoch 105/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2440 - acc: 0.9279\n",
            "Epoch 106/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2453 - acc: 0.9275\n",
            "Epoch 107/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2430 - acc: 0.9273\n",
            "Epoch 108/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2438 - acc: 0.9274\n",
            "Epoch 109/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2415 - acc: 0.9292\n",
            "Epoch 110/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2401 - acc: 0.9292\n",
            "Epoch 111/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2450 - acc: 0.9279\n",
            "Epoch 112/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2475 - acc: 0.9265\n",
            "Epoch 113/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2418 - acc: 0.9288\n",
            "Epoch 114/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2448 - acc: 0.9282\n",
            "Epoch 115/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2418 - acc: 0.9284\n",
            "Epoch 116/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2435 - acc: 0.9283\n",
            "Epoch 117/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2429 - acc: 0.9275\n",
            "Epoch 118/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2406 - acc: 0.9292\n",
            "Epoch 119/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2462 - acc: 0.9278\n",
            "Epoch 120/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2422 - acc: 0.9290\n",
            "Epoch 121/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2443 - acc: 0.9282\n",
            "Epoch 122/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2407 - acc: 0.9288\n",
            "Epoch 123/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2403 - acc: 0.9292\n",
            "Epoch 124/150\n",
            "60000/60000 [==============================] - 3s 48us/sample - loss: 0.2427 - acc: 0.9288\n",
            "Epoch 125/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2403 - acc: 0.9291\n",
            "Epoch 126/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2462 - acc: 0.9279\n",
            "Epoch 127/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2417 - acc: 0.9290\n",
            "Epoch 128/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2395 - acc: 0.9293\n",
            "Epoch 129/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2435 - acc: 0.9286\n",
            "Epoch 130/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2419 - acc: 0.9288\n",
            "Epoch 131/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2402 - acc: 0.9298\n",
            "Epoch 132/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2434 - acc: 0.9286\n",
            "Epoch 133/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2417 - acc: 0.9289\n",
            "Epoch 134/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2421 - acc: 0.9289\n",
            "Epoch 135/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2435 - acc: 0.9274\n",
            "Epoch 136/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2392 - acc: 0.9294\n",
            "Epoch 137/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2408 - acc: 0.9287\n",
            "Epoch 138/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2400 - acc: 0.9298\n",
            "Epoch 139/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2398 - acc: 0.9288\n",
            "Epoch 140/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2402 - acc: 0.9288\n",
            "Epoch 141/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2396 - acc: 0.9283\n",
            "Epoch 142/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2408 - acc: 0.9289\n",
            "Epoch 143/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2410 - acc: 0.9277\n",
            "Epoch 144/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2386 - acc: 0.9288\n",
            "Epoch 145/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2397 - acc: 0.9304\n",
            "Epoch 146/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2420 - acc: 0.9288\n",
            "Epoch 147/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2428 - acc: 0.9284\n",
            "Epoch 148/150\n",
            "60000/60000 [==============================] - 3s 46us/sample - loss: 0.2431 - acc: 0.9289\n",
            "Epoch 149/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2395 - acc: 0.9287\n",
            "Epoch 150/150\n",
            "60000/60000 [==============================] - 3s 47us/sample - loss: 0.2388 - acc: 0.9298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn7gYr3V1N7o",
        "colab_type": "code",
        "outputId": "179b25d3-d94f-481a-b699-6dd6463605ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "score = mnist_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 0s 27us/sample - loss: 0.3360 - acc: 0.9146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nRC0axD1N7y",
        "colab_type": "code",
        "outputId": "e688b8a8-7e0f-4a46-84a3-68f63ad405d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVTkpmJlZlZOCmZlVOSmYmVnV/wfrniCSWOGVqwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MysssR8K1N72",
        "colab_type": "text"
      },
      "source": [
        "## Working on by hand model from yesterday's assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLYNEGrU1N73",
        "colab_type": "code",
        "outputId": "fdb27b17-92dc-427f-e750-1626424d4178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
              "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
              "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
              "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
              "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
              "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
              "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
              "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
              "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
              "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
              "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
              "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
              "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
              "         0.,   0.,   0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82-JEDH81N7-",
        "colab_type": "code",
        "outputId": "09f310d9-594f-4605-cac1-292eeca5ba73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5whu_-wi1N8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        # Set up Architecture of Neural Network\n",
        "        self.input = 784\n",
        "        self.hiddenNodes = 16\n",
        "        self.outputNodes = 10\n",
        "        \n",
        "        # Initial Weights\n",
        "        # 784x16 Matrix Array for the First Layer\n",
        "        self.weights1 = np.random.randn(self.input,self.hiddenNodes)\n",
        "        # 16x10 Matrix Array for Hidden to Output\n",
        "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL3ExICe1N8K",
        "colab_type": "code",
        "outputId": "29269801-e813-4ab1-d601-e2295766a0f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "\n",
        "print(\"Layer 1 weights: \\n\", nn.weights1)\n",
        "print(\"Layer 2 weights: \\n\", nn.weights2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 1 weights: \n",
            " [[-0.55556551  1.34085863  2.30325428 ...  0.40213823  0.34037977\n",
            "  -0.73682421]\n",
            " [ 1.5309029   2.00572164 -0.73580843 ... -1.06279847 -0.9489877\n",
            "   0.08903057]\n",
            " [-0.95656342 -1.14884253  1.33715518 ... -0.28772795  0.17736455\n",
            "   1.46530486]\n",
            " ...\n",
            " [ 1.2375037  -0.43499716 -0.66308861 ...  1.00149795 -0.74504202\n",
            "   1.25030762]\n",
            " [-0.36719709  0.36561068  0.66473998 ... -0.72909737  0.72262694\n",
            "  -0.26693736]\n",
            " [ 0.27073725 -0.53939845  0.46027791 ... -1.49810565  0.1943317\n",
            "  -0.78177083]]\n",
            "Layer 2 weights: \n",
            " [[-0.26118615 -0.60550628 -0.19809427  0.47862116 -1.51514807 -1.65344064\n",
            "   0.41024897  0.29685017 -0.35725428 -1.04867319]\n",
            " [ 0.43409623  0.05921039 -2.01137652  0.89125429 -1.69350264  0.86584317\n",
            "  -1.69167661  1.39235737 -0.95856409 -0.23897538]\n",
            " [-0.50890673 -0.85884013 -0.64763916 -0.79694664  0.57240944 -1.25077051\n",
            "  -1.11937665  0.21671291 -0.02838902 -0.53693663]\n",
            " [-0.34686957  1.02228941 -0.57329003 -0.16278701  0.04750091 -0.54913585\n",
            "   1.81055887 -0.14572562 -0.74777007  1.90739461]\n",
            " [ 0.95025511 -0.47012052  1.02316739 -0.09006183  1.13573637 -0.66350808\n",
            "   0.56280126  0.19109826  1.63861417  1.56683293]\n",
            " [ 0.59074828  0.40854835  0.6434344   1.05936337  0.01460114 -0.84044106\n",
            "  -0.56570358  0.75795937  0.46746352  0.19009755]\n",
            " [ 2.84289747  1.30841502 -0.13534024  2.5734564  -0.75212995 -0.42525498\n",
            "   0.70547953 -1.44684499 -0.68079467  0.63491653]\n",
            " [-0.12982372  0.33982747  1.75206462 -1.38068403  0.20978442  0.28629402\n",
            "  -1.00797564  1.76563477  0.86017718  0.6262563 ]\n",
            " [-0.19445089  0.82890363 -0.42428497 -0.80393365 -0.00507946 -0.07483235\n",
            "   2.27378563  0.00768433  1.20082686 -0.91604642]\n",
            " [ 0.1094744  -0.34810644  0.21045085 -0.49482524  0.24028577 -1.20519466\n",
            "   0.1524813  -1.14573342 -1.81170052 -0.71687156]\n",
            " [ 0.42541421 -1.07055112 -0.8679916  -0.27888595 -1.65710755  1.2860332\n",
            "   0.86809959  0.55226852  1.79534521 -0.73554894]\n",
            " [ 0.51847308  0.43403937  0.3124427   0.30055163  1.22342852 -0.41723232\n",
            "  -0.78909493 -1.37488718 -0.28327603 -1.2686939 ]\n",
            " [ 0.28854004 -2.23409406 -0.85377511  1.36109365 -1.67980066  0.20112703\n",
            "   0.31366929  1.88001155 -1.00324599  0.66046245]\n",
            " [ 1.59088788 -0.99164378 -1.58732469 -1.27093085  0.54355626 -0.64248745\n",
            "   2.26608994 -0.18373518 -2.42414686  1.94454666]\n",
            " [-0.91106774  0.48950927 -0.85859876  1.62852581  0.25984898  0.84260112\n",
            "   0.18335373 -2.93545868  0.45078617  2.01812813]\n",
            " [-0.37416954 -0.56358542  0.44621374 -0.14867031  0.24144201 -0.34442941\n",
            "   0.74844608  0.79310437  1.46457488 -1.26463325]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OB1Maggg1N8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        # Set up Architecture of Neural Network\n",
        "        self.input = 784\n",
        "        self.hiddenNodes = 16\n",
        "        self.outputNodes = 10\n",
        "        \n",
        "        # Initial Weights\n",
        "        #7843x16 Matrix Array for the First Layer\n",
        "        self.weights1 = np.random.randn(self.input,self.hiddenNodes)\n",
        "        # 16x10 Matrix Array for Hidden to Output\n",
        "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1+np.exp(-x))\n",
        "    \n",
        "    def feed_forward(self,X):\n",
        "        \"\"\"\n",
        "        Calculate the NN inference using feed forward.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Weighted sum of inputs & hidden\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "        \n",
        "        # Activations of weighted sum\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        \n",
        "        # Weighted sum between hidden and output\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        # Final Activation of output\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rg_rXQs1N8l",
        "colab_type": "code",
        "outputId": "9d193703-d74c-4a7e-dd1c-ec1dcd14cc9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# generate an output\n",
        "\n",
        "nn = NeuralNetwork()\n",
        "\n",
        "print(X_train[0])\n",
        "output = nn.feed_forward(X_train[0])\n",
        "print(\"output\", output)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
            "  18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
            " 253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
            " 253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
            " 198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
            "  11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
            "   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
            "  70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
            " 225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
            " 240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
            " 229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
            " 253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
            " 253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
            "  80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
            "output [0.00167139 0.03278666 0.10875446 0.97476585 0.85334149 0.98179956\n",
            " 0.99962131 0.2338801  0.99943135 0.02283034]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVQk7bRN1N8y",
        "colab_type": "code",
        "outputId": "8de7f3cb-2f18-4b02-9279-af5226596e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "# calculate error\n",
        "\n",
        "output_all = nn.feed_forward(X_train)\n",
        "error_all = y_train - output_all\n",
        "print(output_all)\n",
        "print(error_all)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.67138840e-03 3.27866636e-02 1.08754459e-01 ... 2.33880098e-01\n",
            "  9.99431353e-01 2.28303412e-02]\n",
            " [2.49533585e-04 3.31742967e-02 9.48553199e-01 ... 3.74109616e-03\n",
            "  7.23320150e-01 6.48038242e-04]\n",
            " [3.11223425e-02 6.08400613e-01 8.27272166e-01 ... 7.12863869e-02\n",
            "  9.43368269e-01 6.92161087e-02]\n",
            " ...\n",
            " [1.48583663e-02 2.93735420e-01 2.56205796e-01 ... 2.84481760e-01\n",
            "  9.88269085e-01 1.51431133e-02]\n",
            " [7.10926406e-03 1.66473882e-01 9.95963154e-01 ... 3.91633327e-03\n",
            "  8.22250699e-01 6.64650980e-03]\n",
            " [9.72493677e-02 6.86913531e-01 9.74174958e-01 ... 4.83727527e-01\n",
            "  4.61917650e-01 2.77725964e-01]]\n",
            "[[-1.67138840e-03 -3.27866636e-02 -1.08754459e-01 ... -2.33880098e-01\n",
            "  -9.99431353e-01 -2.28303412e-02]\n",
            " [ 9.99750466e-01 -3.31742967e-02 -9.48553199e-01 ... -3.74109616e-03\n",
            "  -7.23320150e-01 -6.48038242e-04]\n",
            " [-3.11223425e-02 -6.08400613e-01 -8.27272166e-01 ... -7.12863869e-02\n",
            "  -9.43368269e-01 -6.92161087e-02]\n",
            " ...\n",
            " [-1.48583663e-02 -2.93735420e-01 -2.56205796e-01 ... -2.84481760e-01\n",
            "  -9.88269085e-01 -1.51431133e-02]\n",
            " [-7.10926406e-03 -1.66473882e-01 -9.95963154e-01 ... -3.91633327e-03\n",
            "  -8.22250699e-01 -6.64650980e-03]\n",
            " [-9.72493677e-02 -6.86913531e-01 -9.74174958e-01 ... -4.83727527e-01\n",
            "   5.38082350e-01 -2.77725964e-01]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESmdjobE1N86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        # Set up Architecture of Neural Network\n",
        "        self.input = 784\n",
        "        self.hiddenNodes = 16\n",
        "        self.outputNodes = 10\n",
        "        \n",
        "        # Initial Weights\n",
        "        # 3x4 Matrix Array for the First Layer\n",
        "        self.weights1 = np.random.randn(self.input,self.hiddenNodes)\n",
        "        # 4x1 Matrix Array for Hidden to Output\n",
        "        self.weights2 = np.random.randn(self.hiddenNodes, self.outputNodes)\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1+np.exp(-x))\n",
        "    \n",
        "    def sigmoidPrime(self, x):\n",
        "        return x * (1 - x)\n",
        "    \n",
        "    def feed_forward(self,X):\n",
        "        \"\"\"\n",
        "        Calculate the NN inference using feed forward.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Weighted sum of inputs & hidden\n",
        "        self.hidden_sum = np.dot(X, self.weights1)\n",
        "        \n",
        "        # Activations of weighted sum\n",
        "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
        "        \n",
        "        # Weighted sum between hidden and output\n",
        "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
        "        \n",
        "        # Final Activation of output\n",
        "        self.activated_output = self.sigmoid(self.output_sum)\n",
        "        \n",
        "        return self.activated_output\n",
        "    \n",
        "    def backward(self, X, y, o):\n",
        "        \"\"\"\n",
        "        Backward propagate through the network\n",
        "        \"\"\"\n",
        "        self.o_error = y - o #error in output\n",
        "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
        "        \n",
        "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
        "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
        "        \n",
        "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
        "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        o = self.feed_forward(X)\n",
        "        self.backward(X, y, o)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iitcQceO1N9L",
        "colab_type": "code",
        "outputId": "cdad3814-dba5-467c-834a-61b6bc132210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "\n",
        "for i in range(1000):\n",
        "    if ((i+1) % 100 ==0):\n",
        "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
        "        print('Predicted Output: \\n', str(nn.feed_forward(X_train)))\n",
        "        print(\"Loss: \\n\", str(np.mean(np.square(y_train - nn.feed_forward(X_train)))))\n",
        "    nn.train(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+---------EPOCH 100---------+\n",
            "Predicted Output: \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Loss: \n",
            " 0.1\n",
            "+---------EPOCH 200---------+\n",
            "Predicted Output: \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Loss: \n",
            " 0.1\n",
            "+---------EPOCH 300---------+\n",
            "Predicted Output: \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Loss: \n",
            " 0.1\n",
            "+---------EPOCH 400---------+\n",
            "Predicted Output: \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Loss: \n",
            " 0.1\n",
            "+---------EPOCH 500---------+\n",
            "Predicted Output: \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Loss: \n",
            " 0.1\n",
            "+---------EPOCH 600---------+\n",
            "Predicted Output: \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Loss: \n",
            " 0.1\n",
            "+---------EPOCH 700---------+\n",
            "Predicted Output: \n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "Loss: \n",
            " 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zv_3xNMjzdLI"
      },
      "source": [
        "## Stretch Goals:\n",
        "\n",
        "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
        "- Use Cross Validation techniques to get more consistent results with your model.\n",
        "- Use GridSearchCV to try different combinations of hyperparameters. \n",
        "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
      ]
    }
  ]
}